{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 17\n\n# Initialize bias1\nbias1 = Variable(1.0)\n\n# Initialize weights1 as 3x2 variable of ones\nweights1 = Variable(ones((3, 2)))\n\n# Perform matrix multiplication of borrower_features and weights1\nproduct1 = matmul(borrower_features,weights1)\n\n# Apply sigmoid activation function to product1 + bias1\ndense1 = keras.activations.sigmoid(product1 + bias1)\n\n# Print shape of dense1\nprint(\"\\n dense1's output shape: {}\".format(dense1.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 18\n\n# Compute the product of borrower_features and weights1\nproducts1 = matmul(borrower_features, weights1)\n\n# Apply a sigmoid activation function to products1 + bias1\ndense1 = keras.activations.sigmoid(products1 + bias1)\n\n# Print the shapes of borrower_features, weights1, bias1, and dense1\nprint('\\n shape of borrower_features: ', borrower_features.shape)\nprint('\\n shape of weights1: ', weights1.shape)\nprint('\\n shape of bias1: ', bias1.shape)\nprint('\\n shape of dense1: ', dense1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 19\n\n# Define the first dense layer\ndense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n\n# Define a dense layer with 3 output nodes\ndense2 = keras.layers.Dense(3, activation='sigmoid')(dense1)\n\n# Define a dense layer with 1 output node\npredictions = keras.layers.Dense(1, activation='sigmoid')(dense2)\n\n# Print the shapes of dense1, dense2, and predictions\nprint('\\n shape of dense1: ', dense1.shape)\nprint('\\n shape of dense2: ', dense2.shape)\nprint('\\n shape of predictions: ', predictions.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 20\n\n# Construct input layer from features\ninputs = constant(bill_amounts,float32)\n\n# Define first dense layer\ndense1 = keras.layers.Dense(3, activation='relu')(inputs)\n\n# Define second dense layer\ndense2 = keras.layers.Dense(2, activation='relu')(dense1)\n\n# Define output layer\noutputs = keras.layers.Dense(1, activation='sigmoid')(dense2)\n\n# Print error for first five examples\nerror = default[:5] - outputs.numpy()[:5]\nprint(error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 21\n\n# Construct input layer from borrower features\ninputs = constant(borrower_features,float32)\n\n# Define first dense layer\ndense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n\n# Define second dense layer\ndense2 = keras.layers.Dense(8, activation='relu')(dense1)\n\n# Define output layer\noutputs = keras.layers.Dense(6, activation='softmax')(dense2)\n\n# Print first five predictions\nprint(outputs.numpy()[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 22\n\n# Initialize x_1 and x_2\nx_1 = Variable(6.0,float32)\nx_2 = Variable(0.3,float32)\n\n# Define the optimization operation\nopt = keras.optimizers.SGD(learning_rate=0.01)\n\nfor j in range(100):\n\t# Perform minimization using the loss function and x_1\n\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n\t# Perform minimization using the loss function and x_2\n\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n\n# Print x_1 and x_2 as numpy arrays\nprint(x_1.numpy(), x_2.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 23\n\n# Initialize x_1 and x_2\nx_1 = Variable(0.05,float32)\nx_2 = Variable(0.05,float32)\n\n# Define the optimization operation for opt_1 and opt_2\nopt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\nopt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.0)\n\nfor j in range(100):\n\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n    # Define the minimization operation for opt_2\n\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n\n# Print x_1 and x_2 as numpy arrays\nprint(x_1.numpy(), x_2.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 24\n\n# Define the layer 1 weights\nw1 = Variable(random.normal([23, 7]))\n\n# Initialize the layer 1 bias\nb1 = Variable(ones([7]))\n\n# Define the layer 2 weights\nw2 = Variable(random.normal([7, 1]))\n\n# Define the layer 2 bias\nb2 = Variable(0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 25\n\n# Define the model\ndef model(w1, b1, w2, b2, features = borrower_features):\n\t# Apply relu activation functions to layer 1\n\tlayer1 = keras.activations.relu(matmul(features, w1) + b1)\n    # Apply dropout\n\tdropout = keras.layers.Dropout(0.25)(layer1)\n\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n\n# Define the loss function\ndef loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n\tpredictions = model(w1, b1, w2, b2)\n\t# Pass targets and predictions to the cross entropy loss\n\treturn keras.losses.binary_crossentropy(targets, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 26\n\n# Train the model\nfor j in range(100):\n    # Complete the optimizer\n\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n                 var_list=[w1, b1, w2, b2])\n\n# Make predictions with model\nmodel_predictions = model(w1, b1, w2, b2, test_features)\n\n# Construct the confusion matrix\nconfusion_matrix(test_targets, model_predictions)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}